{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics: Project\n",
    "\n",
    "### Classification Algorithms on the Iris Flower Dataset\n",
    "\n",
    "Author: Daria Sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "***\n",
    "The objective of this project is to explore the use of classification algorithms in supervised learning, focusing on the renowned Iris Flower Dataset introduced by Ronald A. Fisher. \n",
    "\n",
    "The notebook will start with an introduction to supervised learning, explaining its core concepts and significance, followed by a look at classification algorithms. It will then demonstrate the implementation of the Support Vector Machine (SVM) Algorithm using the scikit-learn Python library. Emphasis will be placed on enhancing understanding through appropriate plots, mathematical notation, and diagrams, ensuring a blend of theoretical knowledge and practical application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Supervised Learning\n",
    "\n",
    "Supervised learning falls within the domain of artificial intelligence and machine learning. It is characterised by the use of labelled datasets to train algorithms in classifying data or making accurate predictions (IBM n.d). The goal of supervised learning is for the algorithm to learn a mapping function that can predict the labels for new, unseen data (Brownlee 2023).\n",
    "\n",
    "In supervised learning, we typically have the following components:\n",
    "- **Features** (also referred to as \"X variables\"): These are the input variables or attributes that describe the data instances. For example, in the Iris flower dataset, the features are sepal length, sepal width, petal length, and petal width.\n",
    "- **Labels** (typically referred to as \"target variables\" or \"y variables\"): These are the output variables or categories that we want to predict. In the Iris dataset, the labels correspond to the species of iris flowers (e.g., setosa, versicolor, virginica).\n",
    "- **Training Data**: This is the labelled dataset that we use to train the machine's learning model. It consists of input features paired with the correct output labels. (Ali 2022).\n",
    "\n",
    "In supervised learning, the algorithm is taught by example. An operator provides the machine learning algorithm with a well-defined dataset containing specified inputs and corresponding desired outputs. The algorithm's task is to recognize the underlying patterns that lead to those inputs and outputs. While the operator knows the correct answers to the problem, the algorithm identifies patterns in data, learns from observations and makes predictions. Subsequently, the algorithm produces predictions, which are then reviewed and corrected by the operator. This iterative process continues until the algorithm achieves a high level of accuracy and performance (Wakefield n.d.).\n",
    "\n",
    "<img src=\"./images/sv.png\" width=\"500\" title=\"Superviseed Learning\"/>\n",
    "\n",
    "*Figure 1: Supervised Learning (Baheti 2021)*\n",
    "\n",
    "#### Pros and Cons of Supervised Learning\n",
    "\n",
    "The effectiveness of supervised learning is underscored by its wide applicability and the remarkable advancements it has driven in various fields. From enhancing customer experiences through personalised recommendations to enabling autonomous vehicles, supervised learning algorithms have become indispensable tools in extracting meaningful patterns and insights from complex datasets. Their significance is further magnified by their versatility in handling different types of data and tasks, making them a foundational pillar in the ever-evolving landscape of machine learning and artificial intelligence.\n",
    "\n",
    "Supervised learning, despite being a cornerstone of machine learning, does face significant drawbacks. Its reliance on large volumes of labelled data can be costly and time-consuming, with a risk of inheriting biases from the data. Models are prone to overfitting and lack flexibility, struggling to adapt to new or changing environments. Training, particularly for complex models, demands substantial computational resources. Moreover, ethical concerns and privacy issues arise when handling sensitive data. Despite these issues, supervised learning continues to be a valuable and widely utilised approach in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Algorithms in Supervised Learning\n",
    "\n",
    "##### Classification vs Regression\n",
    "\n",
    "Supervised learning is divided into two main types: **classification** and **regression**. Classification is the task of predicting or identifying which category (or categories) a data point belongs to. In classification output variables are always discrete values meaning they can be placed into clear categories or classes. \n",
    "\n",
    "Unlike classification, which places data into discrete categories, regression problems use input variables to identify continuous, real-value quantities eg. time-series data, sales figures, salaries, scores, heights, weights etc (Hillier 2022).\n",
    "\n",
    "In this project, we will focus on classification, where the goal is to assign each data instance to one of several predefined classes or categories.\n",
    "\n",
    "\n",
    "<img src=\"./images/types.png\" width=\"500\" title=\"Types of Machine Learning\"/>\n",
    "\n",
    "*Figure 2: Types of Machine Learning (Geeksforgeeks n.d.)*\n",
    "\n",
    "##### Classification Algorithms\n",
    "\n",
    "Classification is a type of supervised machine learning technique in which the goal is to accurately predict the label for a given input. The process involves training a model on labeled examples to learn patterns between input features and output classes, followed by an evaluation phase using test data, before being used to perform prediction on new, unseen data (Keita 2022).\n",
    "\n",
    "Examples of Classsification Algorithms include:\n",
    "- **Logistic Regression**: Logistic regression algorithm that models the probability of a binary outcome based on input features, using a logistic function to transform linear combinations of inputs into probabilities. It's commonly used for binary classification tasks, such as spam detection or disease diagnosis\n",
    "\n",
    "- **Support Vector Machine (SVM)**: SVM is a powerful classification method that works by finding the hyperplane that best separates different classes in the feature space. It's effective in high-dimensional spaces and versatile enough to handle linear and non-linear relationships. The algorithm is often used in image classification and bioinformatics.\n",
    "\n",
    "- **Random Forest**: Random forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is commonly used in stock market analysis and e-commerce.\n",
    "\n",
    "- **Decision Tree**: A decision tree is a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. It's simple to understand and interpret, but can be prone to overfitting. Decision trees can be used in customer segmentation and quality control.\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)**: KNN is a simple, non-parametric algorithm that classifies a data point based on how its neighbours are classified, typically using a majority vote of its k nearest neighbours. The algorithm applications include recommendation systems and real estate valuations.\n",
    "\n",
    "- **Naive Bayes**: Naive Bayes is a probabilistic classifier based on applying Bayes' theorem with the assumption of independence between the features. It is particularly suited for high-dimensional data and is known for its simplicity and speed in handling large datasets. It is often used for text classification.\n",
    "\n",
    "(Osisanwo et al. 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of a Support Vector Machine Algorithm\n",
    "***\n",
    "\n",
    "#### Overview\n",
    "\n",
    "Support Vector Machines (SVM) are a set of supervised learning methods used for classification, regression, and outlier detection. They are particularly well-suited for complex but small- or medium-sized datasets. \n",
    "\n",
    "The main objective of SVM is to find the hyperplane that best separates different classes in the feature space by performing optimal data transformations that determine boundaries between data points based on predefined classes, labels, or outputs. In two dimensions, this hyperplane is a line. \n",
    "\n",
    "SVMs are widely used in areas such as image classification, text categorization, bioinformatics, and hand-written character recognition. (Kanade 2022, Geeksforgeeks n.d.).      \n",
    "\n",
    "#### Linear vs Non-Linear SVM\n",
    "\n",
    "Support vector machines are broadly classified into two types: simple or linear SVM and kernel or non-linear SVM.\n",
    "\n",
    "A linear SVM refers to the SVM type used for classifying linearly separable data, meaning that the data points of different classes can be divided by a single straight line. \n",
    "\n",
    "In Linear SVM, **hyperplanes** act as decision-making boundaries, classifying data points based on which side of the hyperplane they fall on. The nature of the hyperplane is determined by the feature count; with two input features, it's a line, and with three, it becomes a two-dimensional plane. \n",
    "\n",
    "A linear SVM is typically used to address classification and regression analysis problems (Kanade 2022)\n",
    "\n",
    "<img src=\"./images/plane.png\" width=\"500\" title=\"Hyperplanes\"/>\n",
    "\n",
    "*Figure 2: 1D and 2D Hyperplanes (Jana 2020)* \n",
    "\n",
    "**Support vectors**, the data points nearest to the hyperplane, significantly impact its placement and direction. By leveraging these support vectors, the classifier's margin is maximised for optimal separation (Gandhi 2008).\n",
    "\n",
    "Non-linear data that cannot be segregated into distinct categories with the help of a straight line uses the *Kernel* trick to handle non-linearly separable data. This involves mapping the input space into a higher-dimensional space where a linear separator can be found. \n",
    "\n",
    "Kernel SVMs are typically used to handle optimization problems that have multiple variables.\n",
    "\n",
    "<img src=\"./images/nonl.png\" width=\"500\" title=\"Linear vs Non-Linear SVM\"/>\n",
    "\n",
    "*Figure 3: Linear vs Non-Linear SVM (Girgin 2019)* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximizing the Margin & Optimisation Problem\n",
    "\n",
    "\n",
    "In the context of Support Vector Machines (SVM), the concept of the margin is integral to understanding its effectiveness. The margin refers to the distance between the hyperplane (the decision boundary) and the nearest data point from either of the classes. The primary objective of an SVM classifier is to locate a hyperplane that not only accurately separates the different classes but also maximises this margin. Essentially, a larger margin equates to a more robust model with better generalisation capabilities. The optimal hyperplane, therefore, is the one that achieves this maximum margin, effectively increasing the classifier's ability to distinguish between classes. \n",
    "\n",
    "Mathematically, the margin is defined as $\\frac{2}{\\|w\\|}$ where $w$ is the weight vector perpendicular to the hyperplane. \n",
    "\n",
    "The mathematical underpinning of SVM involves formulating and solving an optimisation problem. Given a set of training data ${\\bigl\\{(x_1, y_1),...,(x_n, y_n)\\bigl\\}}$, where each $x_i$ is a feature vector and $y_i$ is the class label (either -1 or +1), the objective is to find a hyperplane that separates these classes. The hyperplane is defined by the equation $w*x+b=0$, with $w$ being the weight vector and $b$ the bias\n",
    "\n",
    "The optimisation problem in SVM focuses on minimising $ \\frac{1}{2} \\|w\\|^2 $, which corresponds to maximising the margin, subject to the constraint that each data point $ x_i $ with label $ y_i $ satisfies:\n",
    "\n",
    "$y_i(w \\cdot x_i + b) \\geq 1$\n",
    "\n",
    "This formulation ensures that each data point lies on the correct side of the margin. Maximising the margin is crucial as it leads to a more distinct separation between classes, contributing to the effectiveness and accuracy of the SVM model (Shuzhan 2018).\n",
    "\n",
    "<img src=\"./images/margin.jpg\" width=\"500\" title=\"Maximising the Margin\"/>\n",
    "\n",
    "*Figure 4: Maximising the Margin (Packt n.d.)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Non-Linear Data - Kernel Trick\n",
    "\n",
    "The Kernel trick is a pivotal concept in SVMs, particularly when dealing with non-linear data. This trick involves projecting data into a higher-dimensional space without the need for explicit computation of the coordinates in this new space. By using a kernel function, SVM can effectively classify data that is not linearly separable in the original feature space. Some common kernel functions include Linear, Polynomial, Radial Basis Function (RBF), and Sigmoid. Each kernel has its own way of transforming the data. For instance, the RBF kernel is defined as:\n",
    "\n",
    "$K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)$\n",
    "\n",
    "where $γ$ is a parameter that determines the spread of the kernel. By adjusting this and other parameters, different kernels can be adapted to various data distributions, enabling SVM to handle a wide range of classification challenges (Zhang 2018, Wilimitis 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset Overview and Historical Background\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris Dataset, aslo known as the Fisher's Iris Dataset, is a multivariate dataset created by Sir Ronald Aymer Fisher in 1936. \n",
    "\n",
    "<img src=\"./images/fisher.jpeg\" height=\"300\" title=\"Ronald Aymer Fisher\"/>\n",
    "\n",
    "*Figure 2: R. A. Fisher (Wikipedia 2023)*\n",
    "\n",
    "\n",
    "This dataset is also known as Anderson's Iris dataset, named after Edgar Anderson who collected the data to quantify the variations among Iris flowers of three different classes.\n",
    "\n",
    "The information included in the dataset is as follows:\n",
    "\n",
    "1. Sepal length in cm\n",
    "2. Sepal width in cm\n",
    "3. Petal length in cm\n",
    "4. Petal width in cm\n",
    "5. Class: Iris Setosa, Iris Versicolour, Iris Virginica\n",
    "    \n",
    "<img src=\"./images/iris.png\" width=\"500\" title=\"Iris Species\"/>\n",
    "\n",
    "*Figure 3: Iris Species (Chauhan 2021)*\n",
    "\n",
    "Originally, the dataset served as an example of linear discrimination analysis. However, over time, it gained popularity as a benchmark for evaluating statistical classification methods in machine learning. Today, the Iris Dataset is widely used as an introductory dataset for those starting in machine learning (Chauhan 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Overview\n",
    "\n",
    "***\n",
    "\n",
    "Data quality is crucial in data science and machine learning, as it directly influences machine learning model performance. Data preprocessing, which includes preparing and cleaning data for analysis and modeling, is a critical, not just preliminary, step in these fields (Buhl 2023). \n",
    "\n",
    "Data preprocessing is a detailed process, specific to each dataset and analysis requirement, essential for ensuring data quality and reliability. Effective preprocessing markedly improves machine learning model performance and the quality of analytical insights.\n",
    "\n",
    "Data preprocessing may involve all or some of the following:\n",
    "\n",
    "1. Data Cleaning: handling missing values, removing duplicates, filtering outliers. \n",
    "2. Data Transformation: normalization/standardization, encoding categorical variables.\n",
    "3. Feature Engineering: feature selection and extraction.\n",
    "4. Data Splitting: training and testing split\n",
    "5. Data Balancing: handling imbalanced data\n",
    "6. Data Visualisation: exploratory data analysis.\n",
    "\n",
    "(Gryczka 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset Preprocessing\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading and Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width   class\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.read_csv('csv/iris.csv')\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Displaying general info, checking datatypes and missing values\n",
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Iris Dataset contains 150 entries without any missing values.\n",
    "- The dataset has four numerical features: sepal_length, sepal_width, petal_length, and petal_width, and one categorical feature, class, which represents the species of the iris flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the 'class' column\n",
    "label_encoder = LabelEncoder()\n",
    "iris_df['class'] = label_encoder.fit_transform(iris_df['class'])\n",
    "\n",
    "# Separating the features and the target variable\n",
    "X = iris_df.drop('class', axis=1)\n",
    "y = iris_df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.47393679,  1.22037928, -1.5639872 , -1.30948358],\n",
       "        [-0.13307079,  3.02001693, -1.27728011, -1.04292204],\n",
       "        [ 1.08589829,  0.09560575,  0.38562104,  0.28988568],\n",
       "        [-1.23014297,  0.77046987, -1.21993869, -1.30948358],\n",
       "        [-1.7177306 ,  0.32056046, -1.39196294, -1.30948358]]),\n",
       " 22    0\n",
       " 15    0\n",
       " 65    1\n",
       " 11    0\n",
       " 42    0\n",
       " Name: class, dtype: int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Displaying the first few rows of the scaled training data and the training labels\n",
    "X_train_scaled[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVM Implementation Using scikit-learn\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.95833333 1.         0.83333333 1.         0.95833333]\n",
      "Mean CV Score: 0.95 \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.89      0.94         9\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear')  # Starting with a linear kernel\n",
    "\n",
    "# Performing cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=5)  # Using 5-fold cross-validation\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV Score:\", cv_scores.mean(), \"\\n\")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting using the test set\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluating the model\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Formatting the output\n",
    "formatted_classification_rep = f\"Classification Report:\\n{classification_rep}\"\n",
    "formatted_confusion_matrix = f\"Confusion Matrix:\\n{confusion_mat}\"\n",
    "\n",
    "# Combined formatted output\n",
    "formatted_output = f\"{formatted_classification_rep}\\n\\n{formatted_confusion_matrix}\"\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation\n",
    "\n",
    "\n",
    "The code above demonstrates the process of training, predicting, and evaluating a Support Vector Machine (SVM) model using the scikit-learn library in Python. The following is the step by step explanation of the code:\n",
    "\n",
    "1. Model Initialization: An SVM classifier is initialised with a linear kernel using `SVC(kernel='linear')`. The kernel parameter specifies the type of hyperplane used to separate the data. In this case, 'linear' indicates a linear boundary between the classes.\n",
    "\n",
    "2. Model Training: `fit()` method is used to  train the SVM model on the scaled training data (`X_train_scaled`) and the corresponding labels (`y_train`). The model learns to classify data points based on this training data.\n",
    "\n",
    "3. Making Predictions: After training, the model uses the scaled test data (`X_test_scaled`) to make predictions. The output ``y_pred` is the model's prediction of the class labels for the test set.\n",
    "\n",
    "4. Evaluating the Model: `classification_report()` function is used to generate a report on several key metrics (precision, recall, F1-score) for evaluating the performance of the classification model.\n",
    "\n",
    "5. Formatting the Output: The classification report and confusion matrix outputs are formatted into more readable strings, prefixed with titles for clarity.\n",
    "\n",
    "6. Displaying the Results: The final formatted output, combining both the classification report and the confusion matrix, is concatenated into a single string and printed. This provides a comprehensive view of the model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Interpretation and Model Evaluation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Interpretation\n",
    "\n",
    "**Cross Validation**: The scores represent the model's accuracy on each of the 5 folds used in the cross-validation.\n",
    "The model achieves perfect accuracy (100%) in the second and fourth folds, and 95.8% accuracy in the first and fifth folds. Lower accuracy of approx. 83% is observed in the third fold.\n",
    "\n",
    "The variation in scores suggests that the model's performance is generally high but slightly fluctuates depending on the specific subset of data used in each fold.\n",
    "\n",
    "**Mean CV Score**: This high mean score suggests good generalizability of the model. It implies that the model, when given new data, is likely to perform with similar accuracy.\n",
    "\n",
    "**Classification Report**: The model shows excellent performance, particularly for the Setosa class, which it classified perfectly. The Versicolor and Virginica classes also show high accuracy, though there is a minor misclassification between these two classes.\n",
    "\n",
    "**Confusion Matrix**: The overall accuracy of 97% indicates that the linear kernel SVM is highly effective for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "This SVM model demonstrates robust performance in classifying the species in the Iris dataset. The results highlight the effectiveness of SVM for this type of classification task, especially with a well-preprocessed dataset. The high accuracy and precision also suggest that a linear separation is quite effective for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "***\n",
    "\n",
    "Ali M. (2022). *Supervised Machine Learning.* Available online at <https://www.datacamp.com/blog/supervised-machine-learning>\n",
    "\n",
    "Baheti P. (2021). *Supervised and Unsupervised Learning [Differences & Examples].* <https://www.v7labs.com/blog/supervised-vs-unsupervised-learning\n",
    "\n",
    "Brownlee J. (2022). *Supervised and Unsupervised Machine Learning Algorithms.* Available online at <https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/>\n",
    "\n",
    "Buhl N. (2023). *Mastering Data Cleaning & Data Preprocessing.* Available online at <https://encord.com/blog/data-cleaning-data-preprocessing>\n",
    "\n",
    "Chakure A. (2022). *How to Preprocess Data in Python.* Available online at <https://builtin.com/machine-learning/how-to-preprocess-data-python>\n",
    "\n",
    "Chauhan G. (2021). *Iris Dataset Project from UCI Machine Learning Repository.* Available online at <https://machinelearninghd.com/iris-dataset-uci-machine-learning-repository-project/>\n",
    "\n",
    "Cristianini N. & Shawe-Taylor J. (2000). *An Introduction to Support Vector Machines.* Campridge Univeersity Press.\n",
    "\n",
    "Gandhi R. (2008). *Support Vector Machine — Introduction to Machine Learning Algorithms.* Available online at <https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47>\n",
    "\n",
    "Geeksforgeeks (n.d.). *Support Vector Machine (SVM) Algorithm.* Available online at <https://www.geeksforgeeks.org/support-vector-machine-algorithm/>\n",
    "\n",
    "Geeksforgeeks (n.d.). *Top 10 Algorithms every Machine Learning Engineer should know.* Available onlne at <https://www.geeksforgeeks.org/top-10-algorithms-every-machine-learning-engineer-should-know/>\n",
    "\n",
    "Geeksforgeeks (n.d.). *Types of Machine Learning.* Available online at <https://www.geeksforgeeks.org/types-of-machine-learning/>\n",
    "\n",
    "Girgin S. (2019). *Day-12: Kernel SVM (Non-Linear SVM).* Available online at <https://medium.com/pursuitnotes/day-12-kernel-svm-non-linear-svm-5fdefe77836c>\n",
    "\n",
    "Gryczka K. (2023). *Data preprocessing: a comprehensive step-by-step guide.* Available online at <https://www.future-processing.com/blog/data-preprocessing-a-comprehensive-step-by-step-guide/>\n",
    "\n",
    "Jana A. (2020). *Support Vector Machines for Beginners – Linear SVM.* Available online at <https://www.adeveloperdiary.com/data-science/machine-learning/support-vector-machines-for-beginners-linear-svm/>\n",
    "\n",
    "Hillier W. (2022). *What Is the Difference Between Regression and Classification?* Available online at <https://careerfoundry.com/en/blog/data-analytics/regression-vs-classification/>\n",
    "\n",
    "IBM (n.d.). *What is supervised learning?* Available online at <https://www.ibm.com/topics/supervised-learning>\n",
    "\n",
    "Kanade V. (2022). *What Is a Support Vector Machine? Working, Types, and Examples.* Available online at <https://www.spiceworks.com/tech/big-data/articles/what-is-support-vector-machine/>\n",
    "\n",
    "Keita Z. (2022). *Classification in Machine Learning: An Introduction.* Available online at <https://www.datacamp.com/blog/classification-machine-learning>\n",
    "\n",
    "Osisanwo F.Y., Akinsola J.E.T.,Awodele O., Hinmikaiye J.O., Olakanmi, Akinjobi J. (2017). *Supervised Machine Learning Algorithms: Classification and Comparison.* International Journal of Computer Trends and Technology (IJCTT) – V.48 No.3. Available online at <https://www.researchgate.net/profile/J-E-T-Akinsola/publication/318338750_Supervised_Machine_Learning_Algorithms_Classification_and_Comparison/links/596481dd0f7e9b819497e265/Supervised-Machine-Learning-Algorithms-Classification-and-Comparison.pdf>\n",
    "\n",
    "\n",
    "Packt (n.d.). *SVM: Maximize the Margin.* Available online at <https://static.packt-cdn.com/products/9781783555130/graphics/3547_03_07.jpg>\n",
    "\n",
    "scikit-learn (n.d.). *1.4. Support Vector Machines.* Available online at <https://scikit-learn.org/stable/modules/svm.html>\n",
    "\n",
    "scikit-learn (n.d.). *sklearn.svm.SVC.* Available online at <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>\n",
    "\n",
    "Shuzhan F. (2018). *Understanding the mathematics behind Support Vector Machines.* Available online at <https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/>\n",
    "\n",
    "VanderPlas J. (2016). *Python Data Science Handbook: Essential Tools for Working with Data.* O'Reilly Media, Inc. Available online at <https://jakevdp.github.io/PythonDataScienceHandbook/>\n",
    "\n",
    "Wakefield K. (n.d.). *A guide to the types of machine learning algorithms and their application.* Available online at <https://www.sas.com/en_ie/insights/articles/analytics/machine-learning-algorithms.html>\n",
    "\n",
    "Wikipedia (2023). *Ronald Fisher.* Available online at <https://en.wikipedia.org/wiki/Ronald_Fisher>\n",
    "\n",
    "Wikipedia (2023). *Support Vector Machine.* Available online at https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "Wilimitis D. (2018). *The Kernel Trick in Support Vector Classification.* Available online at <https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f>\n",
    "\n",
    "Zhang G. (2018). *What is the kernel trick? Why is it important?* Available online at <https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
